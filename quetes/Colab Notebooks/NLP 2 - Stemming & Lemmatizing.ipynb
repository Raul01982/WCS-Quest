{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu vas reprendre ton notebook de la quête précédente, puis effectuer les étapes ci-dessous.\n",
    "\n",
    "1/ A partir de ton texte déjà nettoyé, donc en minuscule, sans stopwords ni ponctuation, utilise un stemmer pour rendre le corpus de mots encore plus pertinent, puis utilise FreqDist. Le stemmer a-t-il permis de faire remonter des informations pertinentes en regroupant certains mots de même racine ?\n",
    "\n",
    "2/ A partir de ton texte déjà nettoyé, donc en minuscule, sans stopwords ni ponctuation, utilise un lemmatizer pour rendre le corpus de mots encore plus pertinent, puis utilise FreqDist. Le lemmatizer a-t-il permis de faire remonter des informations pertinentes en regroupant certains mots de même racine ?\n",
    "Attention, pour cette seconde étape, repars du texte nettoyé, mais pas du texte après stemmer. Le but de cet exercice est de comparer les deux méthodes\n",
    "\n",
    "3/ Compare les deux méthodes : les mots avec les plus grandes valeurs dans le FreqDist sont-ils les mêmes ?\n",
    "\n",
    "Partage le lien vers ton notebook en guise de solution au challenge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Un bon feuilleton, que l’on apprécie retrouver chaque semaine, doit offrir son lot de suspense et de surprise. Que penser de l’exercice 2023-2024 de la Ligue 1 conclu ce dimanche soir ? Personne n’est tombé de sa chaise en voyant le Paris SG glaner un 12e titre de champion de France. Reste à savoir ce que sera l’an prochain le visage du club le plus puissant de l’Hexagone sans Kylian Mbappé, son meilleur joueur, en partance pour le Real Madrid. Vaste débat. Pour le reste, l’AS Monaco, quatrième budget de France, a profité de la faiblesse, l’indigence ou l’irrégularité (au choix) de ses concurrents directs que sont Marseille, Lyon, ou encore Rennes et Lens, pour signer son retour en Ligue des champions la saison prochaine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bon',\n",
       " 'feuilleton',\n",
       " 'apprécie',\n",
       " 'retrouver',\n",
       " 'chaque',\n",
       " 'semaine',\n",
       " 'doit',\n",
       " 'offrir',\n",
       " 'lot',\n",
       " 'suspense',\n",
       " 'surprise',\n",
       " 'penser',\n",
       " 'exercice',\n",
       " '2023',\n",
       " '2024',\n",
       " 'ligue',\n",
       " '1',\n",
       " 'conclu',\n",
       " 'dimanche',\n",
       " 'soir',\n",
       " 'personne',\n",
       " 'tombé',\n",
       " 'chaise',\n",
       " 'voyant',\n",
       " 'paris',\n",
       " 'sg',\n",
       " 'glaner',\n",
       " '12e',\n",
       " 'titre',\n",
       " 'champion',\n",
       " 'france',\n",
       " 'reste',\n",
       " 'savoir',\n",
       " 'an',\n",
       " 'prochain',\n",
       " 'visage',\n",
       " 'club',\n",
       " 'plus',\n",
       " 'puissant',\n",
       " 'hexagone',\n",
       " 'sans',\n",
       " 'kylian',\n",
       " 'mbappé',\n",
       " 'meilleur',\n",
       " 'joueur',\n",
       " 'partance',\n",
       " 'real',\n",
       " 'madrid',\n",
       " 'vaste',\n",
       " 'débat',\n",
       " 'reste',\n",
       " 'monaco',\n",
       " 'quatrième',\n",
       " 'budget',\n",
       " 'france',\n",
       " 'a',\n",
       " 'profité',\n",
       " 'faiblesse',\n",
       " 'indigence',\n",
       " 'irrégularité',\n",
       " 'choix',\n",
       " 'concurrents',\n",
       " 'directs',\n",
       " 'marseille',\n",
       " 'lyon',\n",
       " 'encore',\n",
       " 'rennes',\n",
       " 'lens',\n",
       " 'signer',\n",
       " 'retour',\n",
       " 'ligue',\n",
       " 'champions',\n",
       " 'saison',\n",
       " 'prochaine']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "article_1 = tokenizer.tokenize(article.lower())\n",
    "\n",
    "tokens_clean = []\n",
    "for words in article_1:\n",
    "  if words not in nltk.corpus.stopwords.words(\"french\"):\n",
    "    tokens_clean.append(words)\n",
    "\n",
    "tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ligu', 2),\n",
       " ('champion', 2),\n",
       " ('franc', 2),\n",
       " ('rest', 2),\n",
       " ('prochain', 2),\n",
       " ('bon', 1),\n",
       " ('feuilleton', 1),\n",
       " ('apprec', 1),\n",
       " ('retrouv', 1),\n",
       " ('chaqu', 1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stem_en = SnowballStemmer(\"french\")\n",
    "sent_stem = [stem_en.stem(word) for word in tokens_clean]\n",
    "nltk.FreqDist(sent_stem).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ligue', 2),\n",
       " ('champion', 2),\n",
       " ('france', 2),\n",
       " ('reste', 2),\n",
       " ('prochain', 2),\n",
       " (' ', 1),\n",
       " ('bon', 1),\n",
       " ('feuilleton', 1),\n",
       " ('apprécier', 1),\n",
       " ('retrouver', 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "text_clean = \"\"\n",
    "for words in tokens_clean:\n",
    "    text_clean += \" \"\n",
    "    text_clean += words\n",
    "\n",
    "sent_tokens = nlp(text_clean)\n",
    "\n",
    "liste_mots =[]\n",
    "\n",
    "for token in sent_tokens:\n",
    "    # print(token)\n",
    "    liste_mots.append(token.lemma_)\n",
    "\n",
    "nltk.FreqDist(liste_mots).most_common()[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai un resultat trés comparable !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
